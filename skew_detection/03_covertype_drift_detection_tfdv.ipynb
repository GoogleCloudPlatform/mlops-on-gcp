{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-covertype-drift-detection-tfdv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWcjbiI9dkOh",
        "colab_type": "text"
      },
      "source": [
        "# Drift Detection with TensorFlow Data Validation\n",
        "\n",
        "This tutorial shows how to use [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) (TFDV) to identify and analyze different data skews in request-response serving data logged by AI Platform Prediction in BigQuery.\n",
        "\n",
        "The tutorial has three parts:\n",
        "\n",
        "* **Part 1**: Produce baseline statistics and reference schema\n",
        " 1. Download training data\n",
        " 2. Compute baseline statistics from the training data\n",
        " 3. Generate reference schema using the baseline statistics\n",
        "\n",
        "* **Part 2**: Detecting data skews\n",
        " 1. Generate baseline statistics and reference schema from training data using TFDV\n",
        " 2. Read request-response serving data from BigQuery and save it to CSV files\n",
        " 3. Compute statistics from the serving data\n",
        " 4. Validate serving statistics against the reference schema and basline statistics to detect anomalies (if any)\n",
        "\n",
        "* **Part 3**: Analyzing statistics and anomalies\n",
        "  1. Use TFDV to visualize and display the statistics and anomalies\n",
        "  2. Analyze how statistics change over time.\n",
        "\n",
        "\n",
        "We use the [covertype](https://archive.ics.uci.edu/ml/datasets/covertype) from UCI Machine Learning Repository.\n",
        "\n",
        "The dataset is preprocessed, split, and uploaded to the `gs://workshop-datasets/covertype` public GCS location. We use this version of the preprocessed dataset in this notebook. For more information, see [Cover Type Dataset](https://github.com/GoogleCloudPlatform/mlops-on-gcp/tree/master/datasets/covertype).\n",
        "\n",
        "We use the training data split to generate reference schema and statistics from, in order to use for validating serving data.\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoQwXZ7jhA1R",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL_EyB1phEA3",
        "colab_type": "text"
      },
      "source": [
        "### Install packages and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8geaoXthnvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q tensorflow\n",
        "!pip install -U -q tensorflow_data_validation\n",
        "!pip install -U -q pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7igWnNgh0AF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Automatically restart kernel after installs\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZDWGkvAh3ji",
        "colab_type": "text"
      },
      "source": [
        "### Configure GCP environment settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNIw9c6Qh0xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROJECT_ID = \"sa-data-validation\"\n",
        "BUCKET = \"sa-data-validation\"\n",
        "BQ_DATASET_NAME = 'prediction_logs'\n",
        "BQ_VIEW_NAME = 'vw_covertype_classifier_logs_v1'  \n",
        "MODEL_NAME = 'covertype_classifier'\n",
        "MODEL_VERSION = 'v1'\n",
        "!gcloud config set project $PROJECT_ID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-xkckCAiVXk",
        "colab_type": "text"
      },
      "source": [
        "### Authenticate your GCP account\n",
        "\n",
        "This is required if you run the notebook in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzB4OFtWiSY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  print(\"Colab user is authenticated.\")\n",
        "except: pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNZP0LzXiYFz",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmGsur3xiXdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_data_validation as tfdv\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2, statistics_pb2, anomalies_pb2\n",
        "import apache_beam as beam\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import json\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"TF version: {}\".format(tf.__version__))\n",
        "print(\"TFDV version: {}\".format(tfdv.__version__))\n",
        "print(\"Beam version: {}\".format(beam.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ0ZEarvjKNx",
        "colab_type": "text"
      },
      "source": [
        "### Create a local workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAn8OIUJjKTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORKSPACE = './workspace'\n",
        "DATA_DIR = os.path.join(WORKSPACE, 'data')\n",
        "TRAIN_DATA = os.path.join(DATA_DIR, 'train.csv') \n",
        "\n",
        "if tf.io.gfile.exists(WORKSPACE):\n",
        "  print(\"Removing previous workspace artifacts...\")\n",
        "  tf.io.gfile.rmtree(WORKSPACE)\n",
        "\n",
        "print(\"Creating a new workspace...\")\n",
        "tf.io.gfile.makedirs(WORKSPACE)\n",
        "tf.io.gfile.makedirs(DATA_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1dYHVLqwdB",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Generate Baseline Statistics and Reference Schema\n",
        "We use TDV to generate baseline statistics, based on the training data, as well as a reference schema, to validate the serving data against."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoaUj1yZj8Jg",
        "colab_type": "text"
      },
      "source": [
        "## 1. Download data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaCGwr-dkN8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://workshop-datasets/covertype/data_validation/training/dataset.csv {TRAIN_DATA}\n",
        "!wc -l {TRAIN_DATA}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U82cqeblqpHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = pd.read_csv(TRAIN_DATA).head()\n",
        "sample.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE-wWIywq9ZE",
        "colab_type": "text"
      },
      "source": [
        "## 2. Compute baseline statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e8dLW_0qyu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseline_stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=TRAIN_DATA,\n",
        "    stats_options = tfdv.StatsOptions(\n",
        "        sample_count=10000\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2jWSKatq4Vr",
        "colab_type": "text"
      },
      "source": [
        "## 3. Generate reference schema "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "364Hnw6Azo7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reference_schema = tfdv.infer_schema(baseline_stats)\n",
        "\n",
        "# Set Soil_Type to be categorical\n",
        "tfdv.set_domain(reference_schema, 'Soil_Type', schema_pb2.IntDomain(\n",
        "    name='Soil_Type', is_categorical=True))\n",
        "\n",
        "# Set Cover_Type to be categorical\n",
        "tfdv.set_domain(reference_schema, 'Cover_Type', schema_pb2.IntDomain(\n",
        "    name='Cover_Type', is_categorical=True))\n",
        "\n",
        "baseline_stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=TRAIN_DATA,\n",
        "    stats_options=tfdv.StatsOptions(\n",
        "        schema=reference_schema,\n",
        "        sample_count=10000\n",
        "        )\n",
        "    )\n",
        "\n",
        "reference_schema = tfdv.infer_schema(baseline_stats)\n",
        "\n",
        "# Set Soil_Type to be categorical\n",
        "tfdv.set_domain(reference_schema, 'Soil_Type', schema_pb2.IntDomain(\n",
        "    name='Soil_Type', is_categorical=True))\n",
        "\n",
        "# Set Cover_Type to be categorical\n",
        "tfdv.set_domain(reference_schema, 'Cover_Type', schema_pb2.IntDomain(\n",
        "    name='Cover_Type', is_categorical=True))\n",
        "\n",
        "# Set max and min values for Elevation\n",
        "tfdv.set_domain(reference_schema, \n",
        "    'Elevation', \n",
        "    tfdv.utils.schema_util.schema_pb2.IntDomain(\n",
        "        min=1000, \n",
        "        max=5000))\n",
        "\n",
        "# Allow no missing values\n",
        "tfdv.get_feature(reference_schema, \n",
        "    'Slope').presence.min_fraction = 1.0 \n",
        "\n",
        "# Set distribution skew detector for Wilderness_Area\n",
        "tfdv.get_feature(reference_schema, \n",
        "    'Wilderness_Area').skew_comparator.infinity_norm.threshold = 0.05"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVdfArsG34PN",
        "colab_type": "text"
      },
      "source": [
        "Display the reference schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdhRjcl934e7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfdv.display_schema(\n",
        "    schema=reference_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oskOMHmZrJ5I",
        "colab_type": "text"
      },
      "source": [
        "Visualize baseline statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5FoTGPCrIip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfdv.visualize_statistics(baseline_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXgmrq8yMhk5",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Detecting Serving Data Skews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEJUiOf-oNLU",
        "colab_type": "text"
      },
      "source": [
        "## 2. Export Serving Data from BigQuery\n",
        "\n",
        "Although TFDV provides a utility function to calculate statistics on a Pandas dataframe - tfdv.generate_statistics_from_dataframe - that would simplify interactive analysis, the function does not support slicing. Since we need slicing for calculating statistics over different time windows, we will use `tfdv.generate_statistics_from_csv` instead. \n",
        "\n",
        "Thus, we read the request-response serving logs from BigQuery and save the results to CSV files, in order to use `tfdv.generate_statistics_from_csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqmXi69FyuaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TARGET_FEATURE_NAME = 'Cover_Type'\n",
        "FEATURE_NAMES = [feature.name for feature in reference_schema.feature \n",
        "                 if feature.name != TARGET_FEATURE_NAME]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_q7H8qGwvaV",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Read serving data from BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3NpVBYtoPHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_query(source, features, target, start_time, end_time):\n",
        "  query = \"\"\"\n",
        "    SELECT \n",
        "      FORMAT_TIMESTAMP('%Y-%m-%d', time) AS time,\n",
        "      {},\n",
        "      predicted_class AS {}\n",
        "\n",
        "    FROM `{}`\n",
        "    WHERE time BETWEEN '{}' AND '{}'\n",
        "    ;\n",
        "  \"\"\".format(features, target, source, start_time, end_time)\n",
        "\n",
        "  return query"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2isKw0v_rPgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = '2020-05-01 00:00:00 UTC'\n",
        "end_time = '2020-07-01 00:50:00 UTC'\n",
        "\n",
        "source = \"{}.{}\".format(BQ_DATASET_NAME, BQ_VIEW_NAME)\n",
        "features = ', '.join(FEATURE_NAMES)\n",
        "query = generate_query(source, features, TARGET_FEATURE_NAME, start_time, end_time)\n",
        "\n",
        "serving_data = pd.io.gbq.read_gbq(\n",
        "    query, project_id=PROJECT_ID)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiug_MRYsWBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(serving_data.index))\n",
        "serving_data.head(5).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB2HfH0jwyfW",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Save serving data to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C33WbqsvuE4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "serving_data_file = os.path.join(DATA_DIR, 'serving.csv')\n",
        "serving_data.to_csv(serving_data_file, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZeh6qrIxP6C",
        "colab_type": "text"
      },
      "source": [
        "## 3. Compute Statistics from Serving Data\n",
        "\n",
        "In addition to calculating statistics for the full dataset, we also configure TFDV to calculate statistics for each time window"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnhFk1nfxo4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slice_fn = tfdv.get_feature_value_slicer(features={'time': None})\n",
        "\n",
        "serving_stats_list = tfdv.generate_statistics_from_csv(\n",
        "    data_location=serving_data_file,\n",
        "    stats_options=tfdv.StatsOptions(\n",
        "        slice_functions=[slice_fn],\n",
        "        schema=reference_schema\n",
        "        )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRKBotuACAOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slice_keys = sorted([dataset.name for dataset in serving_stats_list.datasets])\n",
        "slice_keys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y461_ATcxpL3",
        "colab_type": "text"
      },
      "source": [
        "## 4. Validate Serving Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flHnqxzMxpSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anomalies_list = []\n",
        "\n",
        "for slice_key in slice_keys[1:]:\n",
        "  serving_stats = tfdv.get_slice_stats(serving_stats_list, slice_key)\n",
        "  anomalies = tfdv.validate_statistics(\n",
        "      serving_stats, \n",
        "      schema=reference_schema, \n",
        "      previous_statistics=baseline_stats\n",
        "  )\n",
        "  anomalies_list.append(anomalies)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcq7J6GpMlU9",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Analyzing Serving Data Statistics and Anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Meuxzarfg09",
        "colab_type": "text"
      },
      "source": [
        "## 1. Visualize Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCf2QyK0ZhCe",
        "colab_type": "text"
      },
      "source": [
        "Visualize statistics for a time window with normal data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0gtTgwnfg7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slice_key = slice_keys[1]\n",
        "serving_stats = tfdv.get_slice_stats(serving_stats_list, slice_key)\n",
        "tfdv.visualize_statistics(\n",
        "    baseline_stats, serving_stats, 'baseline', 'current')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoixrE0BZkez",
        "colab_type": "text"
      },
      "source": [
        "Visualize statistics for a time window with skewed data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE9AmWGzZalz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slice_key = slice_keys[-1]\n",
        "serving_stats = tfdv.get_slice_stats(serving_stats_list, slice_key)\n",
        "tfdv.visualize_statistics(\n",
        "    baseline_stats, serving_stats, 'baseline', 'current')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mesWXuttfgYs",
        "colab_type": "text"
      },
      "source": [
        "## 2. Display Anomalies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f6kULTNZpD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, anomalies in enumerate(anomalies_list):\n",
        "  tfdv.utils.anomalies_util.remove_anomaly_types(\n",
        "      anomalies, [anomalies_pb2.AnomalyInfo.SCHEMA_NEW_COLUMN])\n",
        "  \n",
        "  print(\"Anomalies for  {}\".format(slice_keys[i+1]), )\n",
        "  tfdv.display_anomalies(anomalies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0cX1PbDAyBS",
        "colab_type": "text"
      },
      "source": [
        "## 3. Analyze Statistics Change Over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsVl1Ov1iQs_",
        "colab_type": "text"
      },
      "source": [
        "### 3.1. Numerical feature means over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwWwn_Yli8r2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_features = [\n",
        " feature.steps()[0] \n",
        " for feature in tfdv.utils.schema_util.get_categorical_features(\n",
        "     reference_schema)\n",
        " ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fy3mWl1kbDv",
        "colab_type": "text"
      },
      "source": [
        "Get mean values from baseline statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1CWvdDxkjKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseline_means = dict()\n",
        "for feature in baseline_stats.datasets[0].features:\n",
        "  if feature.path.step[0] == 'time': continue\n",
        "  if feature.path.step[0] not in categorical_features:\n",
        "    mean = feature.num_stats.mean\n",
        "    baseline_means[feature.path.step[0]] = mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBsYaI600bjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "feature_means = defaultdict(list)\n",
        "for slice_key in slice_keys[1:]:\n",
        "  ds = tfdv.get_slice_stats(serving_stats_list, slice_key).datasets[0]\n",
        "  for feature in ds.features:\n",
        "    if feature.path.step[0] == 'time': continue\n",
        "    if feature.path.step[0] not in categorical_features:\n",
        "      mean = feature.num_stats.mean\n",
        "      feature_means[feature.path.step[0]].append(mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIPwhPUECuqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataframe = pd.DataFrame(feature_means, index=slice_keys[1:])\n",
        "num_features = len(feature_means)\n",
        "ncolumns = 3\n",
        "nrows = int(num_features // ncolumns) + 1\n",
        "\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncolumns, figsize=(25, 25))\n",
        "for i, col in enumerate(dataframe.columns[:num_features]):\n",
        "  r = i // ncolumns\n",
        "  c = i % ncolumns\n",
        "  p = dataframe[col].plot.line(ax=axes[r][c], title=col, rot=10)\n",
        "  p.hlines(baseline_means[col], xmin=0, xmax=len(dataframe.index), color='red')\n",
        "  p.text(0, baseline_means[col], 'baseline mean', fontsize=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIJ3_jQLic-m",
        "colab_type": "text"
      },
      "source": [
        "### 3.3. Categorical feature distribution over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8xWyD0-h7JP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_feature_stats = dict()\n",
        "\n",
        "for feature_name in categorical_features:\n",
        "  categorical_feature_stats[feature_name] = dict()\n",
        "\n",
        "  for slice_key in slice_keys[1:]:\n",
        "    categorical_feature_stats[feature_name][slice_key] = dict()\n",
        "    ds = tfdv.get_slice_stats(serving_stats_list, slice_key).datasets[0]\n",
        "    for feature in ds.features:\n",
        "      if feature.path.step[0] == feature_name:\n",
        "        val_freq = list(feature.string_stats.top_values)\n",
        "        for item in val_freq:\n",
        "          categorical_feature_stats[feature_name][slice_key][item.value] = item.frequency\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlDUT_byu0Qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = len(categorical_features)\n",
        "ncolumns = 2\n",
        "nrows = int(num_features // ncolumns) + 1\n",
        "\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncolumns, figsize=(25, 15))\n",
        "for i, feature_name in enumerate(categorical_features):\n",
        "  dataframe = pd.DataFrame(\n",
        "      categorical_feature_stats[feature_name]).T\n",
        "\n",
        "  r = i // ncolumns\n",
        "  c = i % ncolumns\n",
        "  dataframe.plot.bar(ax=axes[r][c], stacked=True, rot=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcvLBWULwiSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}